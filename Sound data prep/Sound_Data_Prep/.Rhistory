left_join(emotion_summary, by = "session_id")
cor.test(vviq_emotion$VVIQ_total, vviq_emotion$emotion_prop, method = "spearman")
balance_summary <- press_events_long %>%
group_by(session_id) %>%
summarise(
E = sum(key == "E"),
I = sum(key == "I"),
total = n(),
emotion_minus_imagery = (E - I) / total,
.groups = "drop"
)
vviq_balance <- data %>%
select(session_id, VVIQ_total) %>%
left_join(balance_summary, by = "session_id")
cor.test(vviq_balance$VVIQ_total, vviq_balance$emotion_minus_imagery, method = "spearman")
library(ggplot2)
ggplot(vviq_imagery, aes(x = VVIQ_total, y = imagery_prop)) +
geom_point(alpha = 0.7) +
geom_smooth(method = "lm", se = TRUE) +
labs(
x = "VVIQ total score",
y = "Proportion of imagery key presses",
title = "Relationship between imagery (VVIQ) and imagery during music"
) +
theme_minimal(base_size = 13)
library(ggplot2)
ggplot(vviq_imagery, aes(x = VVIQ_total, y = imagery_prop)) +
geom_point(alpha = 0.7) +
geom_smooth(method = "lm", se = TRUE) +
labs(
x = "VVIQ total score",
y = "Proportion of imagery key presses",
title = "Relationship between trait imagery (VVIQ) and imagery during music"
) +
theme_minimal(base_size = 13)
library(ggplot2)
ggplot(vviq_imagery, aes(x = VVIQ_total, y = imagery_prop)) +
geom_point(alpha = 0.7) +
geom_smooth(method = "lm", se = TRUE) +
labs(
x = "VVIQ total score",
y = "Proportion of imagery key presses",
title = "Relationship between VVIQ and imagery during music"
) +
theme_minimal(base_size = 13)
ggsave(
"VVIQ_imagery_correlation.png",
width = 8,
height = 6,
dpi = 300
)
imagery_by_song <- press_events_long %>%
group_by(session_id, song_id) %>%
summarise(
imagery_prop = mean(key == "I"),
.groups = "drop"
)
ggplot(imagery_by_song, aes(x = song_id, y = imagery_prop)) +
geom_jitter(width = 0.1, alpha = 0.6, size = 2) +
stat_summary(fun = mean, geom = "point", size = 4, color = "black") +
stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.15) +
labs(
x = "Song emotion",
y = "Proportion of imagery responses",
title = "Imagery responses are more prominent during sad music"
) +
theme_minimal(base_size = 13)
first_press <- press_events_long %>%
group_by(session_id, song_id) %>%
slice_min(time, n = 1) %>%
ungroup()
ggplot(first_press, aes(x = song_id, fill = key)) +
geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(
x = "Song emotion",
y = "Proportion of first responses",
fill = "First response",
title = "Imagery responses occur earlier during sad music"
) +
theme_minimal(base_size = 13)
first_press <- press_events_long %>%
group_by(session_id, song_id) %>%
slice_min(time, n = 1) %>%
ungroup()
ggplot(first_press, aes(x = song_id, fill = key)) +
geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(
x = "Song emotion",
y = "Proportion of first responses",
fill = "First response",
title = "First repsonses"
) +
theme_minimal(base_size = 13)
“The proportion of imagery-first versus emotion-first responses did not differ between happy and sad music. However, analyses of response timing showed that imagery responses occurred earlier overall during sad music, whereas emotional responses occurred earlier during happy music. This suggests that differences between song types emerge in the temporal dynamics of experience rather than in the initial response.”
first_response_summary <- press_events_long %>%
group_by(song_emotion, first_key) %>%
summarise(n = n(), .groups = "drop") %>%
group_by(song_emotion) %>%
mutate(
prop = n / sum(n),
label = paste0(round(prop * 100), "%")
)
View(press_events_long)
library(dplyr)
first_press <- press_events_long %>%
group_by(session_id, song_id) %>%
slice_min(order_by = time, n = 1, with_ties = FALSE) %>%
ungroup() %>%
rename(first_key = key)
first_response_summary <- first_press %>%
count(song_id, first_key, name = "n") %>%
group_by(song_id) %>%
mutate(
prop = n / sum(n),
label = paste0(n, " (", round(prop * 100), "%)")
) %>%
ungroup()
library(ggplot2)
ggplot(first_response_summary, aes(x = song_id, y = prop, fill = first_key)) +
geom_col(width = 0.7) +
geom_text(
aes(label = label),
position = position_stack(vjust = 0.5),
color = "white",
size = 4,
fontface = "bold"
) +
scale_y_continuous(labels = scales::percent) +
labs(
x = "Song emotion",
y = "Proportion of first responses",
fill = "First response",
title = "First response type by song emotion"
) +
theme_minimal(base_size = 13)
View(first_press)
ggplot(first_response_summary,
aes(x = song_id, y = prop, fill = first_key)) +
geom_col(width = 0.7) +
geom_text(
aes(label = label),
position = position_stack(vjust = 0.5),
color = "white",
size = 5,
fontface = "bold"
) +
scale_y_continuous(labels = scales::percent_format()) +
labs(
x = "Song emotion",
y = "Proportion of first responses",
fill = "First response",
title = "First response type by song emotion"
) +
theme_minimal() +
theme(
text = element_text(size = 14),
plot.title = element_text(size = 18, face = "bold"),
axis.title = element_text(size = 15),
axis.text = element_text(size = 14),
legend.title = element_text(size = 14),
legend.text = element_text(size = 13)
)
View(press_events_long)
View(emotions_long)
View(press_table)
View(vviq_balance)
View(vviq_imagery)
View(first_response_summary)
View(first_press)
View(emotion_diversity)
View(balance_summary)
View(data)
View(emo_outline)
View(first_press)
View(first_response_summary)
View(imagery_by_song)
View(imagery_summary)
View(imagery_by_song)
View(first_response_summary)
View(imagery_by_song)
View(imagery_summary)
View(mean_plot)
View(median_plot)
View(outline_offsets)
View(press_events_long)
View(press_events_long)
View(press_gt)
View(press_events_long)
View(press_gt)
View(press_split)
View(press_summary)
View(press_table)
View(vviq_balance)
View(vviq_emotion)
View(data)
final_table <- data %>%
select(
session_id,
song_id,
VVIQ_total,
imagery
) %>%
left_join(emotion_summary,
by = c("session_id", "song_id")) %>%
left_join(first_response_summary,
by = c("session_id", "song_id")) %>%
left_join(imagery_summary,
by = "session_id") %>%
left_join(emotion_prop_summary,
by = "session_id")
base_table <- data %>%
select(session_id, song_id, VVIQ_total) %>%
distinct()
song_level_table <- base_table %>%
left_join(emotion_summary,
by = c("session_id", "song_id")) %>%
left_join(first_response_summary,
by = c("session_id", "song_id")) %>%
left_join(emotion_prop_summary,
by = c("session_id", "song_id"))
names(emotion_summary)
names(first_response_summary)
names(emotion_prop_summary)
emotion_summary_song <- press_events_long %>%
group_by(session_id, song_id) %>%
summarise(
emotion_presses = sum(key == "E"),
total_presses   = n(),
emotion_prop    = emotion_presses / total_presses,
.groups = "drop"
)
imagery_summary_song <- press_events_long %>%
group_by(session_id, song_id) %>%
summarise(
imagery_presses = sum(key == "I"),
total_presses   = n(),
imagery_prop    = imagery_presses / total_presses,
.groups = "drop"
)
first_response_summary_song <- press_events_long %>%
group_by(session_id, song_id) %>%
slice_min(order_by = time, n = 1, with_ties = FALSE) %>%
ungroup() %>%
transmute(session_id, song_id, first_key = key)
base_table <- data %>%
select(session_id, song_id, VVIQ_total, imagery) %>%
distinct()
final_table <- base_table %>%
left_join(emotion_summary_song, by = c("session_id", "song_id")) %>%
left_join(imagery_summary_song, by = c("session_id", "song_id")) %>%
left_join(first_response_summary_song, by = c("session_id", "song_id"))
emo_means <- emotions_long %>%
group_by(session_id, song_id) %>%
summarise(
mean_intensity = mean(intensity, na.rm = TRUE),
mean_valence   = mean(valence, na.rm = TRUE),
.groups = "drop"
)
final_table <- final_table %>%
left_join(emo_means, by = c("session_id", "song_id"))
final_table <- final_table %>%
select(
session_id, song_id, VVIQ_total,
mean_intensity, mean_valence,
first_key,
imagery_prop, emotion_prop,
imagery,
everything()
)
View(final_table)
final_table <- final_table %>%
select(
-emotion_presses,
-imagery_presses,
-total_presses.x,
-total_presses.y
)
Using the median is often better than using the mean because the time-data can be very skewed.
write.csv(final_table,
file = "final_table.csv",
row.names = FALSE)
View(data)
write.csv(final_table,
file = "final_table.csv",
row.names = FALSE)
participants <- data %>%
select(session_id, age, gender) %>%
distinct()
participants %>%
count(gender)
library(ggplot2)
ggplot(participants, aes(x = age)) +
geom_histogram(
binwidth = 2,
fill = "grey70",
color = "black"
) +
labs(
x = "Age (years)",
y = "Number of participants",
title = "Age distribution of participants"
) +
theme_minimal(base_size = 13)
participant_overview <- participants %>%
summarise(
N = n(),
mean_age = round(mean(age, na.rm = TRUE), 1),
sd_age = round(sd(age, na.rm = TRUE), 1),
min_age = min(age, na.rm = TRUE),
max_age = max(age, na.rm = TRUE)
)
gender_table <- participants %>%
count(gender) %>%
mutate(percent = round(n / sum(n) * 100, 1))
View(participant_overview)
View(balance_summary)
View(data)
participant_overview <- participants %>%
summarise(
N = n(),
mean_age = round(mean(age, na.rm = TRUE), 1),
sd_age = round(sd(age, na.rm = TRUE), 1),
min_age = min(age, na.rm = TRUE),
max_age = max(age, na.rm = TRUE)
)
gender_table <- participants %>%
count(gender) %>%
mutate(percent = round(n / sum(n) * 100, 1))
library(dplyr)
age_counts <- participants %>%          # or use your data frame name here
count(age, sort = TRUE) %>%
mutate(percent = round(n / sum(n) * 100, 1))
age_counts
library(dplyr)
gender_table <- participants %>%   # replace with your data frame name
count(gender) %>%
mutate(
Percent = round(n / sum(n) * 100, 1)
) %>%
rename(
Gender = gender,
N = n
)
gender_table
age_table <- participants %>%
count(age) %>%
mutate(
Percent = round(n / sum(n) * 100, 1)
) %>%
rename(
Age_group = age,
N = n
)
age_table
library(gt)
gender_table %>%
gt() %>%
tab_header(title = "Participant gender distribution")
age_table %>%
gt() %>%
tab_header(title = "Participant age distribution")
gender_table %>%
gt() %>%
tab_header(title = "Participant gender distribution")
gender_table %>%
gt() %>%
tab_header(title = "Participant gender distribution")
library(dplyr)
gender_summary <- participants %>%
count(gender) %>%
mutate(
Percent = round(n / sum(n) * 100, 1),
Category = "Gender",
Level = gender
) %>%
select(Category, Level, N = n, Percent)
age_summary <- participants %>%
count(age) %>%
mutate(
Percent = round(n / sum(n) * 100, 1),
Category = "Age group",
Level = age
) %>%
select(Category, Level, N = n, Percent)
combined_table <- bind_rows(gender_summary, age_summary)
library(gt)
participant_table <- combined_table %>%
gt(groupname_col = "Category") %>%
tab_header(
title = "Participant characteristics"
) %>%
cols_label(
Level = "",
N = "n",
Percent = "%"
) %>%
fmt_number(
columns = Percent,
decimals = 1
) %>%
opt_all_caps() %>%
tab_source_note(
source_note = md("Percentages are based on the total sample.")
)
participant_table
gtsave(participant_table, "participant_characteristics.png")
A Spearman rank-order correlation was conducted to examine the relationship between trait imagery ability (VVIQ total score) and the proportion of imagery-related key presses during music listening.
imagery_participants <- press_events_long %>%
group_by(session_id) %>%
summarise(
experienced_imagery = any(key == "I"),
.groups = "drop"
)
sum(imagery_participants$experienced_imagery)
emotion_participants <- press_events_long %>%
group_by(session_id) %>%
summarise(
experienced_emotion = any(key == "E"),
.groups = "drop"
)
sum(emotion_participants$experienced_emotion)
View(press_events_long)
View(press_events_long)
no_imagery_participants <- press_events_long %>%
group_by(session_id) %>%
summarise(
experienced_imagery = any(key == "I"),
.groups = "drop"
) %>%
filter(!experienced_imagery)
no_imagery_participants
no_imagery_vviq <- no_imagery_participants %>%
left_join(
data %>% select(session_id, VVIQ_total),
by = "session_id"
)
no_imagery_vviq
View(data)
View(data)
View(emo_freq)
View(emo_outline)
View(emo_sum)
View(emotion_diversity)
View(emotions_long)
library(dplyr)
library(ggplot2)
# 1) Collapsed VVIQ groups
vviq_groups3 <- data %>%
select(session_id, VVIQ_total) %>%
distinct() %>%
mutate(
vviq_group3 = case_when(
VVIQ_total >= 16 & VVIQ_total <= 48 ~ "Low/Below (16–48)",
VVIQ_total >= 49 & VVIQ_total <= 64 ~ "Average (49–64)",
VVIQ_total >= 65 & VVIQ_total <= 80 ~ "Above/Very high (65–80)",
TRUE ~ NA_character_
),
vviq_group3 = factor(
vviq_group3,
levels = c("Low/Below (16–48)", "Average (49–64)", "Above/Very high (65–80)")
)
)
# 2) First key press per participant per song
first_key_by_song <- press_events_long %>%
group_by(session_id, song_id) %>%
slice_min(order_by = time, n = 1, with_ties = FALSE) %>%
ungroup() %>%
transmute(session_id, song_id, first_key = key)
# 3) Count + proportion within each song_id x vviq_group3
first_response_summary_vviq3 <- first_key_by_song %>%
left_join(vviq_groups3, by = "session_id") %>%
filter(!is.na(vviq_group3)) %>%
group_by(song_id, vviq_group3, first_key) %>%
summarise(n = n(), .groups = "drop") %>%
group_by(song_id, vviq_group3) %>%
mutate(
prop = n / sum(n),
label = paste0(n, " (", round(prop * 100), "%)")
) %>%
ungroup()
# 4) Plot
p_first_vviq3 <- ggplot(first_response_summary_vviq3,
aes(x = song_id, y = prop, fill = first_key)) +
geom_col(width = 0.75) +
geom_text(
aes(label = label),
position = position_stack(vjust = 0.5),
color = "white",
size = 4.5,
fontface = "bold"
) +
scale_y_continuous(labels = scales::percent) +
labs(
title = "First response type by song emotion, split by VVIQ group",
x = "Song emotion",
y = "Proportion of first responses",
fill = "First response"
) +
facet_wrap(~ vviq_group3, nrow = 1) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(size = 20, face = "bold"),
axis.text.x = element_text(size = 14, face = "bold"),
strip.text = element_text(size = 14, face = "bold"),
legend.title = element_text(size = 14),
legend.text = element_text(size = 13)
)
p_first_vviq3
ggsave("first_response_by_vviq_group_collapsed.png", p_first_vviq3,
width = 14, height = 5, dpi = 300)
